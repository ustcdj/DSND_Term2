{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variables\n",
    "\n",
    "One of the main ways for working with categorical variables is using 0, 1 encodings.  In this technique, you create a new column for every level of the categorical variable.  The **advantages** of this approach include:\n",
    "\n",
    "1. The ability to have differing influences of each level on the response.\n",
    "2. You do not impose a rank of the categories.\n",
    "3. The ability to interpret the results more easily than other encodings.\n",
    "\n",
    "The **disadvantages** of this approach are that you introduce a large number of effects into your model.  If you have a large number of categorical variables or categorical variables with a large number of levels, but not a large sample size, you might not be able to estimate the impact of each of these variables on your response variable.  There are some rules of thumb that suggest 10 data points for each variable you add to your model.  That is 10 rows for each column.  This is a reasonable lower bound, but the larger your sample (assuming it is representative), the better.\n",
    "\n",
    "Let's try out adding dummy variables for the categorical variables into the model.  We will compare to see the improvement over the original model only using quantitative variables.  \n",
    "\n",
    "\n",
    "#### Run the cell below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The r-squared score for the model using only quantitative variables was 0.03257139063404435 on 1503 values.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import test1 as t\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./survey_results_public.csv')\n",
    "df.head()\n",
    "\n",
    "#Only use quant variables and drop any rows with missing values\n",
    "num_vars = df[['Salary', 'CareerSatisfaction', 'HoursPerWeek', 'JobSatisfaction', 'StackOverflowSatisfaction']]\n",
    "\n",
    "#Drop the rows with missing salaries\n",
    "drop_sal_df = num_vars.dropna(subset=['Salary'], axis=0)\n",
    "\n",
    "# Mean function\n",
    "fill_mean = lambda col: col.fillna(col.mean())\n",
    "# Fill the mean\n",
    "fill_df = drop_sal_df.apply(fill_mean, axis=0)\n",
    "\n",
    "#Split into explanatory and response variables\n",
    "X = fill_df[['CareerSatisfaction', 'HoursPerWeek', 'JobSatisfaction', 'StackOverflowSatisfaction']]\n",
    "y = fill_df['Salary']\n",
    "\n",
    "#Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42) \n",
    "\n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "        \n",
    "#Predict and score the model\n",
    "y_test_preds = lm_model.predict(X_test) \n",
    "\"The r-squared score for the model using only quantitative variables was {} on {} values.\".format(r2_score(y_test, y_test_preds), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19102, 154)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19102, 147)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df = df.select_dtypes(include=['object'])\n",
    "# Subset to a dataframe only holding the categorical columns\n",
    "\n",
    "# Print how many categorical columns are in the dataframe - should be 147\n",
    "cat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a copy of the dataframe\n",
    "cat_df_copy = cat_df.copy()\n",
    "\n",
    "#Pull a list of the column names of the categorical variables\n",
    "cat_cols_lst = cat_df.columns\n",
    "len(cat_cols_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "## udacity method\n",
    "\n",
    "def create_dummy_df(df, cat_cols, dummy_na):\n",
    "\n",
    "    for col in  cat_cols:\n",
    "        try:\n",
    "            # for each cat add dummy var, drop original column\n",
    "            df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=dummy_na)], axis=1)\n",
    "        except:\n",
    "            continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dj's method (??, sth is not right??)\n",
    "\n",
    "def create_dummy_df(df, cat_cols, dummy_na):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with categorical variables you want to dummy\n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    \n",
    "    OUTPUT:\n",
    "    df - a new dataframe that has the following characteristics:\n",
    "            1. contains all columns that were not specified as categorical\n",
    "            2. removes all the original columns in cat_cols\n",
    "            3. dummy columns for each of the categorical columns in cat_cols\n",
    "            4. if dummy_na is True - it also contains dummy columns for the NaN values\n",
    "            5. Use a prefix of the column name with an underscore (_) for separating \n",
    "    '''\n",
    "    df_cat = df[cat_cols]\n",
    "    df_num = df.drop(columns=cat_cols, axis=1)\n",
    "    df_cat_dummy = pd.get_dummies(df_cat, prefix=cat_cols, prefix_sep='_', dummy_na=dummy_na, columns=cat_cols, drop_first=True)\n",
    "    df = pd.concat([df_num, df_cat_dummy], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5009, 11938)\n"
     ]
    }
   ],
   "source": [
    "df1 = df.dropna(axis=0, how='any', subset=['Salary'])\n",
    "\n",
    "df2 = create_dummy_df(df1, cat_cols_lst, dummy_na=False) #Use your newly created function\n",
    "\n",
    "# Show shape to assure it has a shape of (5009, 11938)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fit_linear_mod(df, response_col, dummy_na=False, test_size=.3, rand_state=42):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - a dataframe holding all the variables of interest\n",
    "    response_col - a string holding the name of the column \n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    test_size - a float between [0,1] about what proportion of data should be in the test dataset\n",
    "    rand_state - an int that is provided as the random state for splitting the data into training and test \n",
    "    \n",
    "    OUTPUT:\n",
    "    test_score - float - r2 score on the test data\n",
    "    train_score - float - r2 score on the test data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    \n",
    "    Your function should:\n",
    "    1. Drop the rows with missing response values\n",
    "    2. Drop columns with NaN for all the values\n",
    "    3. Use create_dummy_df to dummy categorical columns\n",
    "    4. Fill the mean of the column for any missing values \n",
    "    5. Split your data into an X matrix and a response vector y\n",
    "    6. Create training and test sets of data\n",
    "    7. Instantiate a LinearRegression model with normalized data\n",
    "    8. Fit your model to the training data\n",
    "    9. Predict the response for the training data and the test data\n",
    "    10. Obtain an rsquared value for both the training and test data\n",
    "    '''\n",
    "    \n",
    "    df.dropna(axis=0, how='any', subset=[response_col], inplace=True)\n",
    "    print(df.shape)\n",
    "    \n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    print(df.shape)\n",
    "    \n",
    "    ## some category columns were dropped because all NaN\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    df = create_dummy_df(df, cat_cols, dummy_na)\n",
    "    print(df.shape)\n",
    "       \n",
    "    df.fillna(df.mean(), inplace=True, axis=0)\n",
    "    \n",
    "    y = df[response_col]\n",
    "    X = df.drop(columns=[response_col], axis=1)\n",
    "    \n",
    "    #Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_state) \n",
    "\n",
    "    lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "    lm_model.fit(X_train, y_train) #Fit\n",
    "        \n",
    "    #Predict and score the model\n",
    "    y_train_preds = lm_model.predict(X_train)\n",
    "    y_test_preds = lm_model.predict(X_test)\n",
    "    \n",
    "    train_score = r2_score(y_train, y_train_preds)\n",
    "    test_score = r2_score(y_test, y_test_preds)\n",
    "    \n",
    "\n",
    "    return test_score, train_score, lm_model, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5009, 154)\n",
      "(5009, 147)\n",
      "(5009, 11937)\n"
     ]
    }
   ],
   "source": [
    "test_score, train_score, lm_model, X_train, X_test, y_train, y_test = clean_fit_linear_mod(df, 'Salary', dummy_na=False, test_size=.3, rand_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rsquared on the training data was 1.0.  The rsquared on the test data was 0.45306377179469093.\n"
     ]
    }
   ],
   "source": [
    "#Print training and testing score\n",
    "print(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how much higher the rsquared value is on the training data than it is on the test data - why do you think that is?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
